{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        patch_size = config[\"patch_size\"]\n",
    "        num_channels = config[\"num_channels\"] \n",
    "\n",
    "        self.to_patches = nn.Conv2d(in_channels=num_channels,\n",
    "                                    out_channels=config[\"hidden_size\"],\n",
    "                                    kernel_size=patch_size,\n",
    "                                    stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B C H W\n",
    "        x = self.to_patches(x)\n",
    "        # B hidden_size image_size/patch_size image_size/patch_size\n",
    "        x = x.flatten(2)\n",
    "        # B hidden_size (image_size/patch_size)**2 = num_patches\n",
    "        x = x.transpose(1, 2)\n",
    "        # B num_patches hidden_size\n",
    "        return x\n",
    "\n",
    "class ShiftedPatchEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patch_dim = self.patch_size * self.patch_size * self.num_channels * 5\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size),\n",
    "            nn.LayerNorm(self.patch_dim),\n",
    "            nn.Linear(self.patch_dim, self.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shifts = [(1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1)]\n",
    "        shifted_x = [F.pad(x, shift, mode='circular') for shift in shifts]\n",
    "        x_with_shifts = torch.cat([x] + shifted_x, dim=1)\n",
    "        return self.to_patch_tokens(x_with_shifts)\n",
    "\n",
    "def gen_pos_embedding(num_patches, hidden_size):\n",
    "    position_enc = torch.zeros(num_patches, hidden_size)\n",
    "    position = torch.arange(0, num_patches, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-math.log(10000.0) / hidden_size))\n",
    "\n",
    "    position_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    position_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    position_enc = position_enc.unsqueeze(0)\n",
    "    return nn.Parameter(position_enc, requires_grad=False)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = config[\"hidden_size\"] \n",
    "        self.config = config\n",
    "        self.num_patches = (config[\"image_size\"] // config[\"patch_size\"]) ** 2\n",
    "\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # cls: 1 1 hidden_size\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        if (config[\"use_simpleViT\"] == 0):\n",
    "            self.position_embeddings = nn.Parameter(torch.randn(1, self.num_patches + 1, hidden_size))\n",
    "        elif (config[\"use_simpleViT\"] == 1):\n",
    "            self.position_embeddings = gen_pos_embedding(self.num_patches, hidden_size)\n",
    "        # self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        # cls: B 1 hidden_size\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if (self.config[\"use_simpleViT\"] == 0):\n",
    "            x = x + self.position_embeddings\n",
    "        elif (self.config[\"use_simpleViT\"] == 1):\n",
    "            x[:, 1:] += self.position_embeddings\n",
    "        # x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.intermediate_size = config[\"intermediate_size\"]\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_size, self.intermediate_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.intermediate_size, self.hidden_size),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.attention = MultiHeadAttention(config)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=config[\"hidden_size\"],\n",
    "            num_heads=config[\"num_attention_heads\"],\n",
    "            dropout=config[\"attention_probs_dropout_prob\"],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.layernorm_1(x)\n",
    "        attention_output, attention_probs = self.attention(query = norm_x, key = norm_x, value = norm_x)\n",
    "        x = x + attention_output\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        x = x + mlp_output\n",
    "        return (x, attention_probs)\n",
    "  \n",
    "# from performer_pytorch import Performer\n",
    "\n",
    "# class EfficientSelfAttention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.performer = Performer(\n",
    "#             dim=config[\"hidden_size\"],\n",
    "#             depth=1,\n",
    "#             heads=config[\"num_attention_heads\"],\n",
    "#             causal=True,\n",
    "#             dim_head=config[\"hidden_size\"] // config[\"num_attention_heads\"],\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.performer(x)\n",
    "\n",
    "class LocalSelfAttension(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        dim = config[\"hidden_size\"]\n",
    "        heads = config[\"num_attention_heads\"]\n",
    "        dim_head = dim // heads\n",
    "        dropout = config[\"attention_probs_dropout_prob\"]\n",
    "        self.use_performer = config[\"use_performer\"]\n",
    "\n",
    "        inner_dim = dim_head *  heads\n",
    "        # self.efficent_self_attention = EfficientSelfAttention(config)\n",
    "        self.heads = heads\n",
    "        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        # if self.use_performer:\n",
    "        #     x = self.efficent_self_attention(x)\n",
    "        q, k, v = self.prepare_qkv(x)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n",
    "\n",
    "        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n",
    "        mask_value = float('-inf')\n",
    "        dots = dots.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "    \n",
    "    def prepare_qkv(self, x):\n",
    "        qkv = self.to_qkv(x)\n",
    "        return rearrange(qkv, 'b n (h d three) -> three b h n d', h=self.heads, three=3)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_hidden_layers = config[\"num_hidden_layers\"]\n",
    "        self.use_patch_merger = config[\"use_patch_merger\"]\n",
    "        self.patch_merger_index = self.num_hidden_layers // 2\n",
    "        self.use_local_self_attention = config[\"use_local_self_attention\"]\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            if self.use_local_self_attention:\n",
    "                lsa = LocalSelfAttension(config)\n",
    "                self.blocks.append(nn.ModuleList([block, lsa]))\n",
    "            else:\n",
    "                self.blocks.append(block)\n",
    "        self.patch_merger = PatchMerger(dim=config[\"hidden_size\"], num_tokens_out=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_attentions = []\n",
    "        if self.use_local_self_attention:\n",
    "            for block, lsa in self.blocks:\n",
    "                x, attention_probs = block(x)\n",
    "                x = lsa(x) + x\n",
    "                all_attentions.append(attention_probs)\n",
    "            return (x, all_attentions)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, attention_probs = block(x)\n",
    "            all_attentions.append(attention_probs)\n",
    "                \n",
    "            if self.use_patch_merger and i == self.patch_merger_index-1:\n",
    "                x = self.patch_merger(x)\n",
    "        return (x, all_attentions)\n",
    "        \n",
    "class FinalLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.last_layer_use_mlp = config[\"last_layer_use_mlp\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "        if (self.last_layer_use_mlp):\n",
    "            self.classifier = MLP(config)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "        \n",
    "\n",
    "class PatchMerger(nn.Module):\n",
    "    def __init__(self, dim, num_tokens_out):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = nn.Parameter(torch.ones(1) * (dim ** -0.5))  # Learnable scale\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.queries = nn.Parameter(torch.randn(num_tokens_out, dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_tokens_out, 1))  # Bias\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.temp = nn.Parameter(torch.ones(1))  # Learnable T\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        sim = torch.matmul(self.queries, x.transpose(-1, -2)) * self.scale + self.bias\n",
    "        attn = (sim / self.temp).softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        return torch.matmul(attn, x)\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.use_shifted_patch_embeddings = config[\"use_shifted_patch_embeddings\"]\n",
    "        # Create the embedding module\n",
    "        if self.use_shifted_patch_embeddings:\n",
    "            self.embedding = ShiftedPatchEmbeddings(config)\n",
    "        else:\n",
    "            self.embedding = Embeddings(config)\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.classifier = FinalLayer(config)\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=config[\"initializer_range\"])\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "                continue\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "                continue\n",
    "            if isinstance(module, Embeddings):\n",
    "                module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                    module.position_embeddings.data.to(torch.float32),\n",
    "                    mean=0.0,\n",
    "                    std=config[\"initializer_range\"],\n",
    "                ).to(module.position_embeddings.dtype)\n",
    "                module.cls_token.data = nn.init.trunc_normal_(\n",
    "                    module.cls_token.data.to(torch.float32),\n",
    "                    mean=0.0,\n",
    "                    std=config[\"initializer_range\"],\n",
    "                ).to(module.cls_token.dtype)\n",
    "                continue\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output = self.embedding(x)\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output)\n",
    "        logits = self.classifier(encoder_output[:, 0])\n",
    "        return (logits, all_attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skeleton.py, code structure referenced from https://github.com/tintn/vision-transformer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    Used by: Embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Add positional embeddings to the patch embeddings.\n",
    "    Use: PatchEmbeddings\n",
    "    Used by: ViTForClassfication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    Used by: MultiHeadAttention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, attention_head_size, bias=True):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    Use: AttentionHead\n",
    "    Used by: Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    Used by: Block, FinalLayer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    Use: MultiHeadAttention, MLP\n",
    "    Used by: Encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    Use: Block\n",
    "    Used by: ViTForClassfication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The final layer.\n",
    "    Used: MLP\n",
    "    Used by: ViTForClassfication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    Use: Embeddings, Encoder, FinalLayer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=config[\"initializer_range\"])\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "                continue\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "                continue\n",
    "            if isinstance(module, Embeddings):\n",
    "                module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                    module.position_embeddings.data.to(torch.float32),\n",
    "                    mean=0.0,\n",
    "                    std=config[\"initializer_range\"],\n",
    "                ).to(module.position_embeddings.dtype)\n",
    "                module.cls_token.data = nn.init.trunc_normal_(\n",
    "                    module.cls_token.data.to(torch.float32),\n",
    "                    mean=0.0,\n",
    "                    std=config[\"initializer_range\"],\n",
    "                ).to(module.cls_token.dtype)\n",
    "                continue\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py, MODIFIED from https://github.com/tintn/vision-transformer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from data import prepare_data\n",
    "from vit import ViTForClassfication\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    The transformer trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_fn, exp_name, device, config):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.exp_name = exp_name\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "\n",
    "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
    "        train_losses, test_losses, accuracies = [], [], []\n",
    "        for i in range(epochs):\n",
    "            train_loss = self.train_epoch(trainloader)\n",
    "            accuracy, test_loss = self.evaluate(testloader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {\n",
    "                  test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        outdir = os.path.join(\"./experiments\", self.exp_name)\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        configfile = os.path.join(outdir, 'config.json')\n",
    "        with open(configfile, 'w') as f:\n",
    "            json.dump(self.config, f, sort_keys=True, indent=4)\n",
    "        jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "        with open(jsonfile, 'w') as f:\n",
    "            data = {\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'accuracies': accuracies,\n",
    "            }\n",
    "            json.dump(data, f, sort_keys=True, indent=4)\n",
    "        cpfile = os.path.join(outdir, f'model_{epochs}.pt')\n",
    "        torch.save(self.model.state_dict(), cpfile)\n",
    "\n",
    "    def train_epoch(self, trainloader):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in trainloader:\n",
    "            # Move the batch to the device\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            images, labels = batch\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate the loss\n",
    "            loss = self.loss_fn(self.model(images)[0], labels)\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Update the model's parameters\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "        return total_loss / len(trainloader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits, _ = self.model(images)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    metadata = {}\n",
    "    with open('all_tests.json') as f:\n",
    "        all_tests = json.load(f)\n",
    "    for metadata in all_tests:\n",
    "        config = metadata['config']\n",
    "        args = metadata['args']\n",
    "        device = metadata['args']['device']\n",
    "        exp_name = metadata['args']['exp_name']\n",
    "        # Training parameters\n",
    "        batch_size = args[\"batch_size\"]\n",
    "        epochs = args[\"epochs\"]\n",
    "        lr = args[\"lr\"]\n",
    "        device = args[\"device\"]\n",
    "        save_model_every_n_epochs = args[\"save_model_every\"]\n",
    "        # Load the CIFAR10 dataset\n",
    "        trainloader, testloader, _ = prepare_data(batch_size=batch_size)\n",
    "        # Create the model, optimizer, loss function and trainer\n",
    "        model = ViTForClassfication(config)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        trainer = Trainer(model, optimizer, loss_fn,\n",
    "                          args[\"exp_name\"], device=device, config=config)\n",
    "        tic = time.perf_counter()\n",
    "        trainer.train(trainloader, testloader, epochs,\n",
    "                      save_model_every_n_epochs=save_model_every_n_epochs)\n",
    "        toc = time.perf_counter()\n",
    "        seconds_per_epoch = (toc - tic) / epochs\n",
    "        outdir = f\"./experiments/{exp_name}/\"\n",
    "        jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "        with open(jsonfile, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        train_losses = data['train_losses']\n",
    "        test_losses = data['test_losses']\n",
    "        accuracies = data['accuracies']\n",
    "        # Create two subplots of train/test losses and accuracies\n",
    "        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        plt.suptitle(f\"{exp_name}, {seconds_per_epoch:0.4f} seconds per epoch\")\n",
    "        ax1.plot(train_losses, label=\"Train loss\")\n",
    "        ax1.plot(test_losses, label=\"Test loss\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend()\n",
    "        ax2.plot(accuracies)\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        plt.savefig(f\"metrics_{exp_name}.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"config\": {\n",
    "            \"patch_size\": 4,\n",
    "            \"hidden_size\": 48,\n",
    "            \"num_hidden_layers\": 4,\n",
    "            \"num_attention_heads\": 4,\n",
    "            \"intermediate_size\": 192,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"attention_probs_dropout_prob\": 0.0,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"image_size\": 32,\n",
    "            \"num_classes\": 10,\n",
    "            \"num_channels\": 3,\n",
    "            \"qkv_bias\": 1,\n",
    "            \"last_layer_use_mlp\": 0,\n",
    "            \"use_patch_merger\": 0,\n",
    "            \"use_shifted_patch_embeddings\": 0,\n",
    "            \"use_local_self_attention\": 0,\n",
    "            \"use_performer\": 0,\n",
    "            \"use_simpleViT\": 0\n",
    "        },\n",
    "        \"args\": {\n",
    "            \"exp_name\": \"vit_default\",\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 100,\n",
    "            \"lr\": 1e-2,\n",
    "            \"device\": \"cuda\",\n",
    "            \"save_model_every\": 0\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
